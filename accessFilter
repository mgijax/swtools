#!/usr/local/bin/python

# is: a filter through which Apache access.log records can be piped to filter
# 	out various categories of records (sending others out to stdout)

import threading
import pickle
import Queue
import sys
import getopt
import re
import runCommand
import time
import os

USAGE = '''Usage: %s [-r|-s|-i|-m|-g|-v|-b <date>|-a <date>] [-f <filename>] [-o <filename>] [-w <int>] [-d][-c]
	Serves as a filter for Apache access logs.  Can be used to strip out
	various types of requests, based on command-line flags.

    Filtering options:
	-r : exclude hits from robots
	-s : exclude hits from Perl/Python/Java/etc. scripts
	-i : exclude requests for JPG or GIF images
	-m : exclude hits from MGI machines
	-g : exclude hits for GXD pages (often would use this with -v)
	-v : reverse processing (include only those specified, not exclude)
	-b : exclude hits before a given date (mm/dd/yyyy format)
	-a : exclude hits after a given date (mm/dd/yyyy format)
    File-handling options:
	-f : specify an input file from which to read (default is stdin)
	-o : specify an output file to which to write (default is stdout)
    Processing options:
    	-w : number of filtering threads to use (default is 1; 1-10 allowed)
	-d : debug mode (write debugging info to stderr)
	-c : use no-cache mode (resolve all IP addresses without using cache)
    Notes:
    	1. You must use at least one of -r, -s, -i, -m, -g, -b, and -a.
	2. The -a and -b options do not exclude hits from the specified date.
	3. If you use -a and -b with the same date, you will include only the
	   hits from that specific date.
	4. The -v option will include only those records which would have
	   otherwise been excluded by one or more of the specified criteria.
	   (You will get the set of all those which would be excluded without
	   the -v option.)
	5. The default mode starts two additional threads -- one for filtering
	   and one for writing output.  Using additional filtering threads
	   with the -w option should improve performance.  In benchmarking on
	   two multi-processor machines, using '-w 2' seems to be nearly
	   optimal for most cases.
	6. To aid performance, this script uses a temp file in /tmp to track
	   which IP addresses are flagged as being from robots (or not).  Each
	   value is cached for up to a week.  Use -c to avoid this cache and
	   recompute each (and use nslookup for each where needed).
''' % sys.argv[0]

INFILE = sys.stdin
OUTFILE = sys.stdout

START_TIME = time.time()

ROBOTS = False		# boolean flags for which records to exclude
SCRIPTS = False
IMAGES = False
MGI = False
GXD = False

NO_CACHE = False
AFTER_DATE = None
BEFORE_DATE = None
THREAD_COUNT = 1
DEBUG = False

REVERSE = False

ROBOT_TAGS = [ 'bot', 'jeeves', 'inktomi', 'crawl', 'webcapture', 'webcopier',
	'slurp', 'spider', 'httrack', 'httpclient' ]
SCRIPT_TAGS = [ 'python', 'perl', 'java', 'mfc' ]

# mm/dd/yyyy dates from user
USER_RE = re.compile ('([0-9]{2})/([0-9]{2})/([0-9]{4})')

# dd/mmm/yyyy dates from log entries
LOG_RE = re.compile ('([0-9]{2})/([A-Z][a-z]{2})/([0-9]{4})')

def debug (s):
	if DEBUG:
		sys.stderr.write ('%8.3f : %s\n' % (time.time() - START_TIME,
			s) )
	return

def bailout (s):
	sys.stderr.write (USAGE + '\n')
	if s:
		sys.stderr.write ('Error: %s\n' % s)
	sys.exit(1)

abbrevToStr = {
	'Jan' : '01',
	'Feb' : '02',
	'Mar' : '03',
	'Apr' : '04',
	'May' : '05',
	'Jun' : '06',
	'Jul' : '07',
	'Aug' : '08',
	'Sep' : '09',
	'Oct' : '10',
	'Nov' : '11',
	'Dec' : '12',
	}

def logDate (s):
	global LOG_RE

	match = LOG_RE.search (s)
	if not match:
		bailout ('Invalid dd/mmm/yyyy date string: %s' % s)

	year = match.group(3)
	month = match.group(2)
	day = match.group(1)

	if not abbrevToStr.has_key(month):
		bailout ('Invalid month (%s) in date string: %s' % (month, s))

	return year + abbrevToStr[month] + day

def userDate (s):
	global USER_RE

	match = USER_RE.search (s)
	if not match:
		bailout ('Invalid mm/dd/yyyy date string: %s' % s)

	year = match.group(3)
	month = match.group(1)
	day = match.group(2)

	return year + month + day

def processCommandLine():
	global INFILE, OUTFILE, ROBOTS, SCRIPTS, IMAGES, MGI, REVERSE, GXD
	global AFTER_DATE, BEFORE_DATE, THREAD_COUNT, DEBUG, NO_CACHE

	try:
		options, args = getopt.getopt (sys.argv[1:],
			'rsimgvf:o:a:b:w:dc')
	except getopt.GetoptError:
		bailout ('Invalid command-line flag(s)')

	if len(args) > 0:
		bailout ('No extra arguments are allowed')

	for (option, value) in options:
		if option == '-r':
			ROBOTS = True
		elif option == '-s':
			SCRIPTS = True
		elif option == '-i':
			IMAGES = True
		elif option == '-m':
			MGI = True
		elif option == '-v':
			REVERSE = True
		elif option == '-f':
			try:
				INFILE = open (value, 'r')
			except:
				bailout ('Cannot read from: %s' % value)
		elif option == '-o':
			try:
				OUTFILE = open (value, 'w')
			except:
				bailout ('Cannot write to: %s' % value)
		elif option == '-a':
			AFTER_DATE = userDate (value)
		elif option == '-b':
			BEFORE_DATE = userDate (value)
		elif option == '-g':
			GXD = True
		elif option == '-w':
			THREAD_COUNT = int(value)
			if THREAD_COUNT < 1 or THREAD_COUNT > 10:
				bailout ('Thread count (-w) must be ' + \
					'from 1 to 10')
		elif option == '-d':
			DEBUG = True
		elif option == '-c':
			NO_CACHE = True

	if not (ROBOTS or SCRIPTS or MGI or IMAGES or GXD or \
			BEFORE_DATE or AFTER_DATE):
		bailout ('Must use at least one of -r, -s, -i, -m, -g, -a, or -b')
	return

def closeFiles():
	global OUTFILE, INFILE

	if OUTFILE != sys.stdout:
		OUTFILE.close()
	if INFILE != sys.stdin:
		INFILE.close()
	return

def isMgi (ip):
	if ip.startswith('209.222.209'):
	    	return True
	if ip.endswith('informatics.jax.org'):
		return True
	if ip.find('.') == -1:
		return True
	return False

def isImage (uriLower):
	for suffix in [ '.gif', '.jpg', '.jpeg', '.ico' ]:
		if uriLower.endswith(suffix):
			return True

	if uriLower.find ('string_image.cgi') != -1:
		return True

	return False

ROBOT_IP = {}
ROBOT_TIMESTAMP = {}
NUMERIC_RE = re.compile ('^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$')
NAME_RE = re.compile ('Name: *(.*)')

ROBOT_AGENT = {}
SCRIPT_AGENT = {}

def isRobot (ip, agentLower):
	global ROBOT_IP, NUMERIC_RE, NAME_RE, ROBOT_TIMESTAMP, ROBOT_AGENT

	if ROBOT_IP.has_key(ip):
		# if we know this is a robot based on IP address, just report
		# it.  if the IP hasn't flagged it as a robot, then we need to
		# check the user agent.  (the same computer may use a browser
		# sometimes and use robot/scripting sometimes)

		if ROBOT_IP[ip] == True:
			return True
	else:
		address = ip

		# if is numeric address need to resolve to name using nslookup
		# (if possible)

		if NUMERIC_RE.match(ip):
			(stdout, stderr, exitCode) = runCommand.runCommand (
				'nslookup %s' % ip)

			lines = stdout.split ('\n')
			if (len(lines) >= 4) and (lines[3].find('Name') != -1):
				match = NAME_RE.search (lines[3])
				if match:
					address = match.group(1)

		for tag in ROBOT_TAGS:
			if address.find (tag) != -1:
				ROBOT_IP[ip] = True
				ROBOT_TIMESTAMP[ip] = time.time()
				return True

		ROBOT_IP[ip] = False
		ROBOT_TIMESTAMP[ip] = time.time()

	# check user agent

	if ROBOT_AGENT.has_key(agentLower):
		return ROBOT_AGENT[agentLower]

	for tag in ROBOT_TAGS:
		if agentLower.find (tag) != -1:
			ROBOT_AGENT[agentLower] = True
			return True

	ROBOT_AGENT[agentLower] = False
	return False

def isScript (agentLower):
	global SCRIPT_AGENT

	if SCRIPT_AGENT.has_key(agentLower):
		return SCRIPT_AGENT[agentLower]

	for tag in SCRIPT_TAGS:
		if agentLower.find (tag) != -1:
			SCRIPT_AGENT[agentLower] = True
			return True

	SCRIPT_AGENT[agentLower] = False
	return False

def includeDate (dateTime):
	dateStr = logDate(dateTime)
	if AFTER_DATE and dateStr > AFTER_DATE:
		return False
	if BEFORE_DATE and dateStr < BEFORE_DATE:
		return False
	return True 

# strings which identify GXD pages
GXD_TAGS = [ 'expression', 'gxdindex', 'estclone', 'anatdict', 'AMA', 'GEN',
	'markerTissues', 'antibody', 'antigen', '/image.cgi', 'exptools',
	'GXD', 'gxd_tissue_report', 'imageSummaryByMrk', 'ADRefiner',
	'cDNAQF'
	]

def isGxd (uri):
	for tag in GXD_TAGS:
		if uri.find (tag) != -1:
			return True
	return False

def keepLine (line):
	output = True

	fields = line.strip().split(' ')
	numFields = len(fields)

	if ROBOTS or SCRIPTS:
		if numFields >= 12:
			# the agent field is enclosed in quotes and may
			# include spaces, so just put back together everything
			# after field 11; it will be close enough

			agent = ' '.join(fields[11:]).lower()
			if ROBOTS and isRobot(fields[0], agent):
				output = False
			elif SCRIPTS and isScript(agent):
				output = False

	if output and MGI:
		if numFields >= 1:
			if isMgi(fields[0]):
				output = False
	if output and IMAGES:
		if numFields >= 7:
			if isImage(fields[6].lower()):
				output = False

	if output and GXD:
		if numFields >= 7:
			if isGxd(fields[6]):
				output = False

	if output and (BEFORE_DATE or AFTER_DATE):
		if numFields >= 4:
			output = includeDate(fields[3])

	if REVERSE:
		output = not output
	return output

class Filter (threading.Thread):
	def __init__ (self, name, inQueue, outQueue, doneQueue):
		threading.Thread.__init__ (self)
		self.name = name
		self.inQueue = inQueue
		self.outQueue = outQueue
		self.doneQueue = doneQueue
		self.count = 0
		return
	def run (self):
		debug ('started %s' % self.name)
		while True:
			try:
				(lineNum, line) = self.inQueue.get (True, 1)
				self.outQueue.put ( (lineNum, line,
					keepLine(line) ) )
				self.count = self.count + 1
			except Queue.Empty:
				# if no item appears within 1 second, check to
				# see if we have received a "done" signal; if
				# so, exit the thread; if not, sleep a bit and
				# try again

				if not self.doneQueue.empty():
					debug ('%s finished %d lines' % (
						self.name, self.count) )
					return
				debug ('%s waiting...' % self.name)
				time.sleep (0.1)
		return

class Writer (threading.Thread):
	def __init__ (self, outQueue, fp, doneQueue):
		threading.Thread.__init__ (self)
		self.outQueue = outQueue
		self.fp = fp
		self.doneQueue = doneQueue
		self.waiting = {}
		self.nextLine = 0
		return
	def write (self):
		while self.waiting.has_key(self.nextLine):
			(num, line, keep) = self.waiting[self.nextLine]
			if keep:
				self.fp.write (line)
			del self.waiting[self.nextLine]
			self.nextLine = self.nextLine + 1
		return
	def run (self):
		debug ('started writer thread')
		while True:
			try:
				nlk = self.outQueue.get (True, 1)
				self.waiting[nlk[0]] = nlk
				if nlk[0] == self.nextLine:
					self.write ()
			except Queue.Empty:
				# check for a "done" signal; if so, exit the
				# thread; if not, sleep and try again
				if not self.doneQueue.empty():
					return
				debug ('writer thread waiting...')
				time.sleep (0.1)
		return

def filter():
	global INFILE, OUTFILE

	outQueue = Queue.Queue()
	doneQueue = Queue.Queue()
	doneQueue2 = Queue.Queue()
	dataQueue = Queue.Queue()

	writerThread = Writer(outQueue, OUTFILE, doneQueue2)

	filters = []
	dataQueues = []

	for i in range(0, THREAD_COUNT):
#		dataQueues.append (Queue.Queue())
		filters.append (Filter ('filtering thread %d' % i,
			dataQueue, outQueue, doneQueue))
#			dataQueues[-1], outQueue, doneQueue))
		filters[-1].start()

	writerThread.start()

	i = 0
	line = INFILE.readline()
	while line:
#		queue = dataQueues[i % THREAD_COUNT]

		# if the queue is getting large, take a small break
		if dataQueue.qsize() > 10000:
			debug ('main thread waiting...')
			time.sleep(0.25)

		dataQueue.put ( (i, line) )
		i = i + 1
		line = INFILE.readline()

	doneQueue.put ("done")

	i = 0
	for filter in filters:
		filter.join()
		debug ('ended thread %d' % i)
		i = i + 1

	# only end the writer thread after all the filters have ended
	doneQueue2.put ("done")
	writerThread.join()
	debug ('ended writer thread')
	return

def loadCache (file):
	global ROBOT_IP, ROBOT_TIMESTAMP

	if os.path.exists(file):
		fp = open (file, 'r')
		d = pickle.load (fp)
		fp.close()

		ROBOT_IP = d['ip']
		ROBOT_TIMESTAMP = d['timestamp']

		debug ('loaded %d cached IP/robot values' % len(ROBOT_IP))

		now = time.time()
		weekAgo = now - (60 * 60 * 24 * 7)

		items = ROBOT_TIMESTAMP.items()
		for (ip, timestamp) in items:
			if timestamp < weekAgo:
				del ROBOT_IP[ip]
				del ROBOT_TIMESTAMP[ip]

		debug ('kept %d cached IP/robot values' % len(ROBOT_IP))
	return

def saveCache (file):
	global ROBOT_IP, ROBOT_TIMESTAMP

	if not ROBOT_IP:
		return

	try:
		dict = {
			'ip'	    : ROBOT_IP,
			'timestamp' : ROBOT_TIMESTAMP,
			}
		fp = open (file, 'w')
		pickle.dump (dict, fp, 0)
		fp.close()
		runCommand.runCommand ('chmod 666 %s' % file)

		debug ('saved %d IP/robot values to cache' % len(ROBOT_IP))
	except:
		debug ('failed to write new IP/robot cache file')
	return

def main():
	robotCache = '/tmp/accessFilter.robots'

	processCommandLine()
	if not NO_CACHE:
		loadCache (robotCache)
	filter()
	closeFiles()
	if not NO_CACHE:
		saveCache (robotCache)

if __name__ == '__main__':
	main()
