#!/usr/local/bin/python

# is: a filter through which Apache access.log records can be piped to filter
# 	out various categories of records (sending others out to stdout)

import subprocess
import pickle
import sys
import getopt
import re
import runCommand
import time
import os
import socket

USAGE = '''Usage: %s [-r|-s|-i|-j|-C|-m|-g|-v|-b <date>|-a <date>] [-f <filename>] [-o <filename>] [-w <int>] [-d][-c][-x]
	Serves as a filter for Apache access logs.  Can be used to strip out
	various types of requests, based on command-line flags.

    Filtering options:
	-r : exclude hits from robots
	-s : exclude hits from Perl/Python/Java/etc. scripts
	-i : exclude requests for JPG or GIF images
	-j : exclude requests for Javascript files
	-C : exclude requests for CSS files
	-m : exclude hits from MGI machines
	-g : exclude hits for GXD pages (often would use this with -v)
	-v : reverse processing (include only those specified, not exclude)
	-b : exclude hits before a given date (mm/dd/yyyy format)
	-a : exclude hits after a given date (mm/dd/yyyy format)
    File-handling options:
	-f : specify an input file from which to read (default is stdin)
	-o : specify an output file to which to write (default is stdout)
    Processing options:
    	-w : number of filtering subprocesses use (default is 1; 1-12 allowed)
	-d : debug mode (write debugging info to stderr)
	-c : use no-cache mode (resolve all IP addresses without using cache)
    Internal options:
    	-x : for internal use only; set up as subprocess for parallelization
    Notes:
    	1. You must use at least one of -r, -s, -i, -m, -g, -b, and -a.
	2. The -a and -b options do not exclude hits from the specified date.
	3. If you use -a and -b with the same date, you will include only the
	   hits from that specific date.
	4. The -v option will include only those records which would have
	   otherwise been excluded by one or more of the specified criteria.
	   (You will get the set of all those which would be excluded without
	   the -v option.)
	5. The default mode starts four subprocesses for filtering.  This is
	   the sweet-spot that showed up during testing, showing a 10-84%%
	   performance improvement over the prior multi-threaded version
	   (depending on the data set).  From 1-12 subprocesses is allowed.
	6. To aid performance, this script uses a temp file in /tmp to track
	   which IP addresses are flagged as being from robots (or not).  Each
	   value is cached for up to a week.  Use -c to avoid this cache and
	   recompute each (and resolve the address for each where needed).
''' % sys.argv[0]

INFILE = sys.stdin
OUTFILE = sys.stdout

START_TIME = time.time()

ROBOTS = False		# boolean flags for which records to exclude
SCRIPTS = False
IMAGES = False
JAVASCRIPT = False
CSS = False
MGI = False
GXD = False

NO_CACHE = False
AFTER_DATE = None
BEFORE_DATE = None
THREAD_COUNT = 4
DEBUG = False
SUBPROCESS = False

REVERSE = False

ROBOT_TAGS = [ 'bot', 'jeeves', 'inktomi', 'crawl', 'webcapture', 'webcopier',
	'slurp', 'spider', 'httrack', 'httpclient' ]
SCRIPT_TAGS = [ 'python', 'perl', 'java', 'mfc' ]

# mm/dd/yyyy dates from user
USER_RE = re.compile ('([0-9]{2})/([0-9]{2})/([0-9]{4})')

# dd/mmm/yyyy dates from log entries
LOG_RE = re.compile ('([0-9]{2})/([A-Z][a-z]{2})/([0-9]{4})')

def debug (s):
	if DEBUG:
		sys.stderr.write ('%8.3f : %s\n' % (time.time() - START_TIME,
			s) )
	return

def bailout (s):
	sys.stderr.write (USAGE + '\n')
	if s:
		sys.stderr.write ('Error: %s\n' % s)
	sys.exit(1)

abbrevToStr = {
	'Jan' : '01',
	'Feb' : '02',
	'Mar' : '03',
	'Apr' : '04',
	'May' : '05',
	'Jun' : '06',
	'Jul' : '07',
	'Aug' : '08',
	'Sep' : '09',
	'Oct' : '10',
	'Nov' : '11',
	'Dec' : '12',
	}

def logDate (s):
	global LOG_RE

	match = LOG_RE.search (s)
	if not match:
		bailout ('Invalid dd/mmm/yyyy date string: %s' % s)

	year = match.group(3)
	month = match.group(2)
	day = match.group(1)

	if not abbrevToStr.has_key(month):
		bailout ('Invalid month (%s) in date string: %s' % (month, s))

	return year + abbrevToStr[month] + day

def userDate (s):
	global USER_RE

	match = USER_RE.search (s)
	if not match:
		bailout ('Invalid mm/dd/yyyy date string: %s' % s)

	year = match.group(3)
	month = match.group(1)
	day = match.group(2)

	return year + month + day

def processCommandLine():
	global INFILE, OUTFILE, ROBOTS, SCRIPTS, IMAGES, MGI, REVERSE, GXD
	global AFTER_DATE, BEFORE_DATE, THREAD_COUNT, DEBUG, NO_CACHE
	global JAVASCRIPT, CSS, SUBPROCESS

	try:
		options, args = getopt.getopt (sys.argv[1:],
			'rsimgvxf:o:a:b:w:dcjC')
	except getopt.GetoptError:
		bailout ('Invalid command-line flag(s)')

	if len(args) > 0:
		bailout ('No extra arguments are allowed')

	for (option, value) in options:
		if option == '-r':
			ROBOTS = True
		elif option == '-s':
			SCRIPTS = True
		elif option == '-x':
			SUBPROCESS = True
		elif option == '-i':
			IMAGES = True
		elif option == '-m':
			MGI = True
		elif option == '-v':
			REVERSE = True
		elif option == '-f':
			try:
				INFILE = open (value, 'r')
			except:
				bailout ('Cannot read from: %s' % value)
		elif option == '-o':
			try:
				OUTFILE = open (value, 'w')
			except:
				bailout ('Cannot write to: %s' % value)
		elif option == '-a':
			AFTER_DATE = userDate (value)
		elif option == '-b':
			BEFORE_DATE = userDate (value)
		elif option == '-j':
			JAVASCRIPT = True
		elif option == '-C':
			CSS = True
		elif option == '-g':
			GXD = True
		elif option == '-w':
			THREAD_COUNT = int(value)
			if THREAD_COUNT < 1 or THREAD_COUNT > 12:
				bailout ('Subprocess count (-w) must be ' + \
					'from 1 to 12')
		elif option == '-d':
			DEBUG = True
		elif option == '-c':
			NO_CACHE = True

	if not (ROBOTS or SCRIPTS or MGI or IMAGES or GXD or \
			BEFORE_DATE or AFTER_DATE or JAVASCRIPT or CSS):
		bailout ('Must use at least one of -r, -s, -i, -j, ' + \
			'-C, -m, -g, -a, or -b')
	return

def closeFiles():
	global OUTFILE, INFILE

	if OUTFILE != sys.stdout:
		OUTFILE.close()
	if INFILE != sys.stdin:
		INFILE.close()
	return

def isMgi (ip):
	if ip.startswith('209.222.209'):
	    	return True
	if ip.endswith('informatics.jax.org'):
		return True
	if ip.find('.') == -1:
		return True
	return False

def isImage (uriLower):
	for suffix in [ '.gif', '.jpg', '.jpeg', '.ico' ]:
		if uriLower.endswith(suffix):
			return True

	if uriLower.find ('string_image.cgi') != -1:
		return True

	return False

ROBOT_IP = {}
ROBOT_TIMESTAMP = {}
NUMERIC_RE = re.compile ('^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$')
NAME_RE = re.compile ('Name: *(.*)')

ROBOT_AGENT = {}
SCRIPT_AGENT = {}
NEW_ROBOTS = []
BOT_REPORTS = {}

def isRobot (ip, agentLower):
	global ROBOT_IP, NUMERIC_RE, NAME_RE, ROBOT_TIMESTAMP, ROBOT_AGENT
	global NEW_ROBOTS

	if ROBOT_IP.has_key(ip):
		# if we know this is a robot based on IP address, just report
		# it.  if the IP hasn't flagged it as a robot, then we need to
		# check the user agent.  (the same computer may use a browser
		# sometimes and use robot/scripting sometimes)

		if ROBOT_IP[ip] == True:
			return True
	else:
		address = ip

		# if is numeric address, we need to resolve to name 
		# (if possible)

		if NUMERIC_RE.match(ip):
			address = socket.getfqdn(ip)

#			(stdout, stderr, exitCode) = runCommand.runCommand (
#				'nslookup %s' % ip)
#			lines = stdout.split ('\n')
#			if (len(lines) >= 4) and (lines[3].find('Name') != -1):
#				match = NAME_RE.search (lines[3])
#				if match:
#					address = match.group(1)

		for tag in ROBOT_TAGS:
			if address.find (tag) != -1:
				ROBOT_IP[ip] = True
				ROBOT_TIMESTAMP[ip] = time.time()
				NEW_ROBOTS.append ( (ip, 1) )
				return True

		ROBOT_IP[ip] = False
		ROBOT_TIMESTAMP[ip] = time.time()
		NEW_ROBOTS.append ( (ip, 0) )

	# check user agent

	if ROBOT_AGENT.has_key(agentLower):
		return ROBOT_AGENT[agentLower]

	for tag in ROBOT_TAGS:
		if agentLower.find (tag) != -1:
			ROBOT_AGENT[agentLower] = True
			return True

	ROBOT_AGENT[agentLower] = False
	return False

def isScript (agentLower):
	global SCRIPT_AGENT

	if SCRIPT_AGENT.has_key(agentLower):
		return SCRIPT_AGENT[agentLower]

	for tag in SCRIPT_TAGS:
		if agentLower.find (tag) != -1:
			SCRIPT_AGENT[agentLower] = True
			return True

	SCRIPT_AGENT[agentLower] = False
	return False

def includeDate (dateTime):
	dateStr = logDate(dateTime)
	if AFTER_DATE and dateStr > AFTER_DATE:
		return False
	if BEFORE_DATE and dateStr < BEFORE_DATE:
		return False
	return True 

# strings which identify GXD pages
GXD_TAGS = [ 'expression', 'gxdindex', 'estclone', 'anatdict', 'AMA', 'GEN',
	'markerTissues', 'antibody', 'antigen', '/image.cgi', 'exptools',
	'GXD', 'gxd_tissue_report', 'imageSummaryByMrk', 'ADRefiner',
	'cDNAQF'
	]

def isGxd (uri):
	for tag in GXD_TAGS:
		if uri.find (tag) != -1:
			return True
	return False

def keepLine (line):
	output = True

	fields = line.strip().split(' ')
	numFields = len(fields)

	if ROBOTS or SCRIPTS:
		if numFields >= 12:
			# the agent field is enclosed in quotes and may
			# include spaces, so just put back together everything
			# after field 11; it will be close enough

			agent = ' '.join(fields[11:]).lower()
			if ROBOTS and isRobot(fields[0], agent):
				output = False
			elif SCRIPTS and isScript(agent):
				output = False
		else:
			# no agent field, but we can still check the IP
			if ROBOTS and isRobot(fields[0], ''):
				output = False

	if output and JAVASCRIPT:
		if numFields >= 7:
			if fields[6][-3:] == '.js':
				output = False

	if output and CSS:
		if numFields >= 7:
			if fields[6][-4:] == '.css':
				output = False

	if output and MGI:
		if numFields >= 1:
			if isMgi(fields[0]):
				output = False
	if output and IMAGES:
		if numFields >= 7:
			if isImage(fields[6].lower()):
				output = False

	if output and GXD:
		if numFields >= 7:
			if isGxd(fields[6]):
				output = False

	if output and (BEFORE_DATE or AFTER_DATE):
		if numFields >= 4:
			output = includeDate(fields[3])

	if REVERSE:
		output = not output
	return output

QUIT = '*quit*'
REPORT = '*report*'

def write (outQueue):
	global NEW_ROBOTS
	done = False

	for (ip, flag) in NEW_ROBOTS:
		OUTFILE.write ('*%s %s*\n' % (ip, flag))

	NEW_ROBOTS = []
	OUTFILE.flush()

	for line in outQueue:
		OUTFILE.write(line)
		OUTFILE.write ('\n')
	OUTFILE.flush()
	return

def subprocFilter():
	outQueue = []

	OUTFILE.write(REPORT + '\n')
	OUTFILE.flush()

	line = INFILE.readline().rstrip()
	while line:
		if (line[0] != '*') or (line[-1] != '*'):
			if keepLine(line):
				outQueue.append (line)
		elif line.startswith(QUIT):
			return
		elif line.startswith(REPORT):
			outQueue.append (REPORT)
			write (outQueue)
			outQueue = []
		else:
			items = line[1:-1].split(' ')
			ip = items[0]
			code = (items[1] == '1')

			if not ROBOT_IP.has_key(ip):
				ROBOT_IP[ip] = code
				ROBOT_TIMESTAMP[ip] = time.time()

		line = INFILE.readline().rstrip()
	return

def addWorker (processes):
	skipnext = False
	subargs = []
	for item in sys.argv:
		if item in ('-f', '-o'):
			skipnext = True
		elif skipnext:
			skipnext = False
		else:
			subargs.append (item)

	proc = subprocess.Popen (subargs + ['-x'],
		stdin=subprocess.PIPE,
		stdout=subprocess.PIPE)
	processes.append (proc)
	return proc

def subprocIO (buckets, processes, procRange):
	# gather any existing results, then send out the new buckets

	global OUTFILE, ROBOT_IP, ROBOT_TIMESTAMP, BOT_REPORTS

	debug ('send buckets: %s' % str(map(len,buckets)))

	for i in procRange:
		# collect a new set of isRobot flags from this process
		BOT_REPORTS[i] = []

		process = processes[i]

		# get output from previous run
		line = process.stdout.readline()

		ct = 0

		# continue until we reach the REPORT termination flag
		while not line.startswith(REPORT):

			# regular log lines get written out
			if (line[0] != '*'):
				OUTFILE.write(line)
				ct = ct + 1

			# is-robot flag lines get parsed and remembered
			else:
				items = line[1:-2].split(' ')
				ip = items[0]
				code = (items[1] == '1')

				ROBOT_IP[ip] = code
				ROBOT_TIMESTAMP[ip] = time.time()
				BOT_REPORTS[i].append ( (ip, items[1]) )

			line = process.stdout.readline()

		debug ('proc %d sent %d lines, %d bot flags' % \
			(i, ct, len(BOT_REPORTS[i]) ))
		OUTFILE.flush()

		# send input to subprocesses

		# if more lines waiting in bucket, send next set of inputs
		if buckets[i]:

			# send along is-robot flags we collected last round
			for (procid, robots) in BOT_REPORTS.items():
				if procid == i:
					continue
				for (ip, flag) in robots:
					process.stdin.write ('*%s %s*\n' % (
						ip, flag))
				process.stdin.flush()

			# send new log records
			for line in buckets[i]:
				process.stdin.write (line)
			process.stdin.flush()

		# send flag requesting a report for those records
		process.stdin.write (REPORT + '\n')
		process.stdin.flush()

		# reset this bucket
		buckets[i] = []
		debug ('sent lines to proc %d' % i)
	return

def filter(procCount = 5):
	global INFILE 

	procRange = range(0, procCount)
	processes = []
	buckets = []

	for i in procRange:
		addWorker(processes)
		buckets.append ([])

	ct = 0
	i = 0
	cacheSize = 1000
	bucketSize = cacheSize / procCount
	maxBucket = procCount - 1

	line = INFILE.readline()
	while line:
		pn = min(maxBucket, ct / bucketSize)
		buckets[pn].append (line)

		i = i + 1
		ct = ct + 1
		if ct >= cacheSize:
			subprocIO (buckets, processes, procRange)
			ct = 0
			debug ('total lines sent: %d' % i)

		line = INFILE.readline()

	debug ('finished reading data')

	if ct > 0:
		# send final buckets
		subprocIO (buckets, processes, procRange)

	# gather final results
	subprocIO (buckets, processes, procRange)
	
	for process in processes:
		process.stdin.write (QUIT + '\n')
		process.stdin.flush()
		process.stdin.close()
		process.wait()
		debug ('subprocess %d ended' % process.pid)
	return

def loadCache (file):
	global ROBOT_IP, ROBOT_TIMESTAMP

	if os.path.exists(file):
		fp = open (file, 'r')
		d = pickle.load (fp)
		fp.close()

		ROBOT_IP = d['ip']
		ROBOT_TIMESTAMP = d['timestamp']

		debug ('loaded %d cached IP/robot values' % len(ROBOT_IP))

		now = time.time()
		weekAgo = now - (60 * 60 * 24 * 7)

		items = ROBOT_TIMESTAMP.items()
		for (ip, timestamp) in items:
			if timestamp < weekAgo:
				del ROBOT_IP[ip]
				del ROBOT_TIMESTAMP[ip]

		debug ('kept %d cached IP/robot values' % len(ROBOT_IP))
	return

def saveCache (file):
	global ROBOT_IP, ROBOT_TIMESTAMP

	if not ROBOT_IP:
		return

	try:
		dict = {
			'ip'	    : ROBOT_IP,
			'timestamp' : ROBOT_TIMESTAMP,
			}
		fp = open (file, 'w')
		pickle.dump (dict, fp, 0)
		fp.close()
		runCommand.runCommand ('chmod 666 %s' % file)

		debug ('saved %d IP/robot values to cache' % len(ROBOT_IP))
	except:
		debug ('failed to write new IP/robot cache file')
	return

def main():
	robotCache = '/tmp/accessFilter.robots'

	processCommandLine()
	if not NO_CACHE:
		loadCache (robotCache)

	if not SUBPROCESS:
		filter(THREAD_COUNT)
		closeFiles()
		if not NO_CACHE:
			saveCache (robotCache)
		debug ('main process ended')
	else:
		subprocFilter()

if __name__ == '__main__':
	main()
