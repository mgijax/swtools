#!/usr/local/bin/python

# monitors the state of locks within a Sybase server, collecting data at
# specified intervals over a specified period of time, then writing or
# emailing a report at the end of that period.

import sys
sys.path.insert (0, '/usr/local/mgi/live/lib/python')
import os
import getopt
import db
import subprocess
import time
import types
import socket
import mgi_utils

###--- Global variables ---###

INTERVAL = 120		# seconds between checks
DURATION = 8		# duration of test period (in hours)
SERVER = None		# name of Sybase database server
EMAIL = None		# address to which to email a report
LOGDIR = None		# directory to which to log a report

TEST_TIME = time.time()	# time at which the current test started
SUMMARY = []		# list of lists; used to generate summary table
BY_TABLE = []		# list of lists; used to generate details by table
BY_SPID = []		# list of lists; used to generate details by spid

ERRORS = []		# list of strings; non-fatal errors which occurred
ALL = 'all'		# constant; used to group counts for all lock types
FORMAT = 'text'		# output format text or tab

USAGE = '''Usage: %s -S <db server> [-i <interval>] [-d <duration>] [-e <email>] [-l <log dir>] [-t]
    Purpose:
	Monitors the state of locking in a specified Sybase database,
	collecting data at specified intervals over a specified time period,
	then emailing or writing a report.  Intended to help track down which
	processes are using many locks, which tables are hot spots, etc.
    Options:
	-S : required; which Sybase database server
	-i : optional; number of seconds between checks (default is %d)
	-d : optional; number of hours in test period (default is %d)
	-e : optional; email address to which to send a report
	-l : optional; directory in which to log a report file
	-t : optional; write a tab-delimited report, rather than a plain text
		one (for importing into Excel rather than human readability)
    Notes:
    	1. You must use either -e or -l or both.
	2. It is recommended to run this as a background process.
''' % (sys.argv[0], INTERVAL, DURATION)

###--- Functions ---###

def bailout (msg, showUsage=False, isError=True):
	if not sys.stderr.isatty():
		fp = open('tracklocks.failure.log', 'w')
		fp.write (msg)
		fp.close()
		sys.exit(1)

	if showUsage:
		sys.stderr.write (USAGE)
	if isError:
		sys.stderr.write ('Error: %s\n' % msg)
	else:
		sys.stderr.write ('Report: %s\n' % msg)
		sys.exit(0)
	sys.exit(1)

def logError (e):
	# collect non-fatal errors to display at once later on
	global ERRORS
	if e not in ERRORS:
		ERRORS.append (e)
	return

def processCommandLine ():
	global SERVER, INTERVAL, DURATION, EMAIL, LOGDIR, FORMAT
	try:
		options, args = getopt.getopt (sys.argv[1:], 'S:i:d:e:l:t')
	except getopt.GetoptError:
		bailout ('Improper command-line option(s)', True)

	for (option, value) in options:
		if option == '-S':
			SERVER = value
		elif option == '-i':
			INTERVAL = int(value)
		elif option == '-d':
			DURATION = int(value)
		elif option == '-e':
			EMAIL = value
		elif option == '-l':
			LOGDIR = value
		elif option == '-t':
			FORMAT = 'tab'
		else:
			bailout ('Invalid option: %s' % option, True)

	if not SERVER:
		bailout ('Must specify database server using -S', True)
	if not (EMAIL or LOGDIR):
		bailout ('Must specify either -e or -l for reporting', True)

	db.set_sqlLogin ('mgd_public', 'mgdpub', SERVER, 'master')
	return

nameCache = {}
def nslookup (ip):
	global nameCache

	if nameCache.has_key(ip):
		return nameCache[ip]
	fqdn = socket.getfqdn(ip)
	if fqdn:
		nameCache[ip] = fqdn
	return fqdn

def querySybase ():
	# gather what information we can from Sybase

	# gather info from Sybase on three items as close together as possible
	# to minimize the chance of changes between queries

	# 1. basic data from sp_who
	try:
		sp_who_results = db.sql ('sp_who', 'auto')
	except:
		bailout ('Fatal database error: sp_who failed')

	# 2. more detailed data from sysprocesses
	try:
		sysprocesses_results = db.sql ('''SELECT spid, status,
			hostname, program_name, hostprocess, cmd, ipaddr
			FROM sysprocesses''', 'auto')
	except:
		logError ('Database error: select from sysprocesses failed')
		return

	# 3. locking data from sp_lock
	try:
		sp_lock_results = db.sql ('sp_lock', 'auto')
	except:
		bailout ('Fatal database error: sp_lock failed')

	# check for no activity

	if not sp_who_results:
		bailout ('No processes on server', False, False)

	# now collect the data from the queries 

	processes = {}

	for row in sp_who_results:
		# tweak standard Sybase processes, which have no login
		if row['loginame'] == None:
			row['loginame'] = 'sybase'

		processes[row['spid']] = {
			'database' : row['dbname'],
			'database user' : row['loginame'],
			}

	for row in sysprocesses_results:
		if processes.has_key(row['spid']):
			process = processes[row['spid']]
			process['database access method'] = \
				row['program_name']
			process['database command'] = row['cmd']
			process['database spid'] = row['spid']
			process['database status'] = row['status']
			process['host'] = row['hostname']
			process['host IP'] = row['ipaddr']
			process['host process'] = row['hostprocess']

	lockSummary = {}	# lockSummary[lock type] = count
	locksByTable = {}	# locksByTable[object id][lock type] = count
	locksBySpid = {}	# locksBySpid[spid][object id][lock type] = ct

	lockSummary[ALL] = 0

	for row in sp_lock_results:
		lockType = row['locktype']
		objectID = '%s..%d' % (row['dbname'], row['table_id'])
		spid = row['spid']

		# overall summary

		if lockSummary.has_key(lockType):
			lockSummary[lockType] = lockSummary[lockType] + 1
		else:
			lockSummary[lockType] = 1

		lockSummary[ALL] = lockSummary[ALL] + 1

		# details organized by table

		if locksByTable.has_key(objectID):
			if locksByTable[objectID].has_key(lockType):
				locksByTable[objectID][lockType] = 1 + \
					locksByTable[objectID][lockType]
			else:
				locksByTable[objectID][lockType] = 1
		else:
			locksByTable[objectID] = { lockType : 1 }

		if not locksByTable[objectID].has_key(ALL):
			locksByTable[objectID][ALL] = 0
		locksByTable[objectID][ALL] = locksByTable[objectID][ALL] + 1

		# details organized by spid

		if locksBySpid.has_key(spid):
			if locksBySpid[spid].has_key(objectID):
				if locksBySpid[spid][objectID].has_key(lockType):
				    locksBySpid[spid][objectID][lockType] = \
					1 + \
					locksBySpid[spid][objectID][lockType]
				else:
					locksBySpid[spid][objectID][lockType] = 1
			else:
				locksBySpid[spid][objectID] = { lockType : 1 }
		else:
			locksBySpid[spid] = { objectID : { lockType : 1 } }

		if not locksBySpid[spid].has_key(ALL):
			locksBySpid[spid][ALL] = { ALL : 0 }
		locksBySpid[spid][ALL][ALL] = locksBySpid[spid][ALL][ALL] + 1

		if not locksBySpid[spid][ALL].has_key(lockType):
			locksBySpid[spid][ALL][lockType] = 0
		locksBySpid[spid][ALL][lockType] = 1 + \
			locksBySpid[spid][ALL][lockType] 

		if not locksBySpid[spid][objectID].has_key(ALL):
			locksBySpid[spid][objectID][ALL] = 0
		locksBySpid[spid][objectID][ALL] = 1 + \
			locksBySpid[spid][objectID][ALL] 

	return processes, lockSummary, locksByTable, locksBySpid

def processPsOutput (tpl, host, ip, processes):
	# extract data from the output of subprocess (in tpl, a tuple) for
	# the given host/ip and update processes

	(stdout, stderr, exitcode) = tpl
	if exitcode:
		if host and len(host.strip()) > 0:
			logError ('Cannot run ps on %s' % host)
		elif ip and len(ip.strip()) > 0:
			logError ('Cannot run ps on %s' % ip)
		return

	dict = {}
	if type(stdout) == types.ListType:
		lines = stdout[1:]
	else:
		lines = stdout.split ('\n')[1:]		# skip header line
	for line in lines:
		fields = line.split()
		if len(fields) >= 8:
			hostprocess = int(fields[1])
			dict[hostprocess] = {
				'OS command' : ' '.join(fields[7:]),
				'OS parent process' : fields[2],
				'OS process' : fields[1],
				'OS process start time' : fields[4],
				'OS user' : fields[0],
				}

			# fix older processes with date-based timestamp
			if len(fields[4]) == 3:
				dict[hostprocess]['OS process start time'] = \
					fields[4] + ' ' + fields[5]
				dict[hostprocess]['OS command'] = \
					' '.join(fields[8:])

	# fill data into processes
	for (key, process) in processes.items():
		# if the process if from the host we are inquiring about, then
		# that's one we should try to update

		if (process.has_key('host') and (process['host'] == host)) \
		or (process.has_key('host IP') and process['host IP'] == ip):
			pid = process['host process'].strip()
			try:
				if dict.has_key(int(pid)):
					 process.update (dict[int(pid)])
			except:
				# if something goes wrong with the conversion
				# or lookup, just move on
				pass
	return

failedHosts = {}
def querySolaris (processes):
	global failedHosts

	# bail out if we don't have any host info
	anyHost = False
	for (key, process) in processes.items():
		if process.has_key('host') or process.has_key('host IP'):
			anyHost = True
			break

	if not anyHost:
		logError ('Insufficient data from database to tie to OS')
		return

	# get the set of the hosts we need to check { IP : host name }
	hosts = {}
	for (key, process) in processes.items():
		# if we have an IP, use that as the key
		if process.has_key('host IP') and \
		    not hosts.has_key(process['host IP']):
			hosts[process['host IP']] = process['host']

		# otherwise, if we have a host name, use that as the key
		elif process.has_key('host') and \
		    not hosts.has_key(process['host']):
			hosts[process['host']] = process['host']

	# use rsh to remotely execute ps on solaris boxes, if possible
	for (ip, host) in hosts.items():
		if ip and ip.startswith('209.222'):
			addr = nslookup(ip)
		elif host:
			addr = host
		else:
			continue

		# if we already had a failure for this host, skip it
		if failedHosts.has_key(addr):
			return

		proc = subprocess.Popen (['/usr/bin/rsh', addr, 'ps -aef' ],
			stdout=subprocess.PIPE,
			stderr=subprocess.PIPE)
		proc.wait()

		exitCode = proc.returncode
		stdout = proc.stdout.readlines()
		stderr = proc.stdout.readlines()

		# if we failed, then don't check this host again
		if exitCode:
			failedHosts[addr] = True

		processPsOutput ( (stdout, stderr, exitCode), host, addr,
			processes)
	return

lookupCache = { ALL : ALL }
def lookup (objectID):
	global lookupCache

	if not lookupCache.has_key(objectID):
		[ dbName, tableID ] = objectID.split('..')

		results = db.sql ('''select name, id, type
			from %s..sysobjects
			where id = %s''' % (dbName, tableID), 'auto')

		if len(results) == 0:
			lookupCache[objectID] = '%s..unknown' % dbName
		else:
			lookupCache[objectID] = '%s..%s (%s)' % (dbName,
				results[0]['name'],
				results[0]['type'].strip()) 

	return lookupCache[objectID]

def timestamp():
	return time.strftime ('%m-%d-%y %H:%M:%S', time.localtime(TEST_TIME))

def tableRows (locksByTable, indent = '  '):
	tableRows = []

	# tables = [ (table name, total count, { lock type : count }), ... ]
	tables = []

	for objectId in locksByTable.keys():
		dict = locksByTable[objectId]
		tables.append( (dict[ALL], lookup(objectId), dict) )

	tables.sort()		# most-locked tables to appear first
	tables.reverse()
	
	for (allCount, tableName, dict) in tables:
		tableRow = [ indent + tableName ]
		for lockType in lockTypes:
			if dict.has_key(lockType):
				tableRow.append(dict[lockType])
			else:
				tableRow.append('')
		tableRows.append (tableRow)
	return tableRows

def spidInfo (spid, processes):
	# builds a 2-line description of the spid's process

	if not processes.has_key(spid):
		return None

	p = processes[spid]

	# line 1 -- database information

	out1 = []
	if p.has_key('database user'):
		out1.append ('user ' + p['database user'].strip())

	if p.has_key('database'):
		out1.append (' in ' + p['database'].strip())

	if p.has_key('database access method'):
		if p['database access method'].strip():
			out1.append (' via ' + \
				p['database access method'].strip())

	if p.has_key('database command'):
		out1.append (', command ' + p['database command'].strip())

	if p.has_key('database status'):
		out1.append (', status ' + p['database status'].strip())

	line1 = '  Database Info: ' + ''.join (out1)

	# line 2 -- OS information

	out2 = []
	if p.has_key('OS user'):
		out2.append ('user ' + p['OS user'].strip())

	if p.has_key('host'):
		if p['host'].strip():
			out2.append (' from ' + p['host'].strip())
	elif p.has_key('host IP'):
		if p['host IP'].strip():
			out2.append (' from ' + p['host IP'].strip())
		
	if p.has_key('host process'):
		if p['host process'].strip():
			out2.append (', PID ' + p['host process'].strip())
	elif p.has_key('OS process'):
		if p['OS process'].strip():
			out2.append (', PID ' + p['OS process'].strip())

	if p.has_key('OS parent process'):
		if p['OS parent process'].strip():
			out2.append (', parent PID ' + \
				p['OS parent process'].strip())

	if p.has_key('OS process start time'):
		out2.append (', process started ' + \
			p['OS process start time'])
	if p.has_key('OS command'):
		out2.append (', running "' + p['OS command'] + '"')

	line2 = '  OS Info: ' + ''.join(out2)
	
	return [ line1, line2 ]

lockTypes = [ ALL ]
def record (processes, lockSummary, locksByTable, locksBySpid):
	global SUMMARY, BY_TABLE, BY_SPID, lockTypes

	# pick up any as-yet-unseen lock types and add them to the existing
	# order

	theseLockTypes = lockSummary.keys()
	theseLockTypes.sort() 

	for lockType in theseLockTypes:
		if lockType not in lockTypes:
			lockTypes.append (lockType)

	# add a row to the overall summary

	summaryRow = [ timestamp() ]
	for lockType in lockTypes:
		if lockSummary.has_key(lockType):
			summaryRow.append (lockSummary[lockType])
		else:
			summaryRow.append ('')

	SUMMARY.append (summaryRow)

	# copy that summary row into the by-table list and then add a row
	# for each table with its details

	BY_TABLE.append (summaryRow)
	BY_TABLE = BY_TABLE + tableRows(locksByTable)

	# copy the summary row into the by-spid list and then add rows for
	# each spid and its tables

	BY_SPID.append (summaryRow)

	spids = []
	for (key, value) in locksBySpid.items():
		spids.append ( (value[ALL][ALL], key) )

	spids.sort()		# spids from most-locked to least
	spids.reverse()

	for (count, spid) in spids:
		spidRow = [ '  spid ' + str(spid) ]
		dict = locksBySpid[spid][ALL]

		for lockType in lockTypes:
			if dict.has_key(lockType):
				spidRow.append(dict[lockType])
			else:
				spidRow.append('')
		BY_SPID.append (spidRow)
		info = spidInfo(spid, processes)
		if info:
			BY_SPID = BY_SPID + info
		BY_SPID = BY_SPID + tableRows(locksBySpid[spid], '    ')
	return

def buildTable (header, rows):
	out = []

	if FORMAT == 'tab':
		out.append ('\t'.join(header))
		for row in rows:
			if type(row) == types.ListType:
				out.append ('\t'.join(map(str,row)))
			else:
				out.append (row)

	elif FORMAT == 'text':
		# find the maximum length for each column, excluding rows
		# with the wrong number of columns (like 1-line rows)

		maxes = map (lambda x : len(x) + 1, header)
		for row in rows:
			if type(row) == types.StringType:
				continue
			i = 0
			while i < len(row):
				if len(str(row[i])) >= maxes[i]:
					maxes[i] = len(str(row[i])) + 1
				i = i + 1

		# left-align the left-most column and right-align the others

		line = header[0].ljust(maxes[0])
		i = 0
		for head in header[1:]:
			i = i + 1
			line = line + head.rjust(maxes[i])
		out.append (line)

		for row in rows:
			if type(row) == types.ListType:
				line = row[0].ljust(maxes[0])
				i = 0
				for count in row[1:]:
					i = i + 1
					line = line + \
						str(count).rjust(maxes[i])
				out.append(line)
			else:
				out.append(row)

	return '\n'.join(out)

def buildReport():
	divider = '-' * 40
	out = [ divider ]

	if ERRORS:
		out.append ('Error Messages:')
		out = out + ERRORS
		out.append (divider)

	header = [ 'Time', ] + lockTypes
	out.append ('Summary')
	out.append (buildTable (header, SUMMARY))
	out.append (divider)

	header = [ 'Time/Table' ] + header[1:]
	out.append ('Details by Table')
	out.append (buildTable (header, BY_TABLE))
	out.append (divider)

	header = [ 'Time/SPID/Table' ] + header[1:]
	out.append ('Details by SPID, then by Table')
	out.append (buildTable (header, BY_SPID))
	out.append (divider)

	return '\n'.join (out)

def writeReport (report):
	global LOGDIR

	suffix = time.strftime ('%y-%d-%m-%H-%M-%S',
		time.localtime(TEST_TIME))

	file = os.path.join (LOGDIR, 'tracklocks-' + suffix)
	try:
		fp = open (file, 'w')
		fp.write(report)
		fp.close()
	except:
		if LOGDIR != '/tmp':
			LOGDIR = '/tmp'
			writeReport(report)
			bailout ('Could not write report to %s, wrote to %s instead' % (file, LOGDIR))
		else:
			bailout ('Could not write report to %s' % LOGDIR) 
	return

def emailReport (report):
	global LOGDIR

	try:
		mgi_utils.send_Mail ('tracklocks', EMAIL,
			'Lock report, started %s' % timestamp(), report)
	except:
		if not LOGDIR:
			LOGDIR = '/tmp'
			writeReport(report)
		bailout (
		'Failed to send email to %s, wrote log file to /tmp' % EMAIL)
	return

def main ():
	global TEST_TIME
	processCommandLine()

	endTime = TEST_TIME + (3600.0 * DURATION)
	nextTime = TEST_TIME + INTERVAL

	while TEST_TIME <= endTime:
		processes, lockSummary, locksByTable, locksBySpid = \
			querySybase()
		querySolaris(processes)
		record(processes, lockSummary, locksByTable, locksBySpid)

		if time.time() > nextTime:
			bailout ('Interval is too short; tests overlap',
				isError = True)

		TEST_TIME = nextTime
		nextTime = nextTime + INTERVAL
		time.sleep (TEST_TIME - time.time())

	report = buildReport()

	if LOGDIR:
		writeReport(report)
	if EMAIL:
		emailReport(report)
	return

if __name__ == '__main__':
	try:
		main()
	except:
		fp = open('tracklocks.failure.log', 'w')
		import traceback
		traceback.print_exc (file = fp)
		fp.close()
